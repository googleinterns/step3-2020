{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Process_csv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QjSIYLFPUok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e7783ec-6dad-4534-fe26-82d43099e5bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Jns0OYPYfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def read_files(files):\n",
        "  df = pd.concat(map(pd.read_csv, files))\n",
        "  return df\n",
        "\n",
        "def process_link(link):\n",
        "  # remove orgs with malformed url\n",
        "  if link.find('.') == -1:\n",
        "    return ''\n",
        "  # exclude facebook/guidestar/wikipedia links\n",
        "  if 'facebook' in link or 'guidestar' in link or 'wikipedia' in link:\n",
        "    return ''\n",
        "  index = link.find('â€º')\n",
        "  if index > -1:\n",
        "    link = link[: index - 1]\n",
        "  return link\n",
        "\n",
        "# Name: only first letter is capitalized\n",
        "# Link: remove Facebook/guidestar\n",
        "# Remove special characters to keep only valid URLs\n",
        "# About: Change to sentence case\n",
        "def process_text(df):\n",
        "  output = []\n",
        "  for _, org in df.iterrows(): \n",
        "    name = org['name'].title()\n",
        "    link = org['link']\n",
        "    about_input = org['about']\n",
        "    # remove orgs without url or about\n",
        "    if type(link) == str and type(about_input) == str:\n",
        "      url = process_link(str(link))\n",
        "    else:\n",
        "      continue\n",
        "    if not url:\n",
        "      continue\n",
        "    # about = re.sub(r'\\d+', '', about_input.capitalize())\n",
        "    about = about_input.capitalize()\n",
        "    output.append([name, url, about])\n",
        "  return output\n",
        "\n",
        "def save_results(results, start, stop):\n",
        "  with open('/content/drive/My Drive/Capstone/processed_data/processed_irs990_' + str(start) + '_' + str(stop) + '.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for data in results:\n",
        "      writer.writerow(data)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTL8rHh8P7D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = 0\n",
        "stop = 5000\n",
        "step = 50\n",
        "files = ['/content/drive/My Drive/Capstone/scraped_data/links_irs990_' + str(i)\n",
        "    + '_' + str(i + step) + '.csv' for i in range(start, stop, step)]\n",
        "df = read_files(files)\n",
        "processed = process_text(df)\n",
        "save_results(processed, start, stop)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3gl7OfVVByV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This only works when deployed on GCP\n",
        "from google.cloud import language_v1\n",
        "from google.cloud.language_v1 import enums\n",
        "\n",
        "\n",
        "def classify_text(text_content):\n",
        "    \"\"\"\n",
        "    Classifying Content in a String\n",
        "\n",
        "    Args:\n",
        "      text_content The text content to analyze. Must include at least 20 words.\n",
        "    \"\"\"\n",
        "\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "\n",
        "    # Available types: PLAIN_TEXT, HTML\n",
        "    type_ = enums.Document.Type.PLAIN_TEXT\n",
        "\n",
        "    # Optional. If not specified, the language is automatically detected.\n",
        "    # For list of supported languages:\n",
        "    # https://cloud.google.com/natural-language/docs/languages\n",
        "    language = \"en\"\n",
        "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
        "\n",
        "    response = client.classify_text(document)\n",
        "    # Loop through classified categories returned from the API\n",
        "    for category in response.categories:\n",
        "        # Get the name of the category representing the document.\n",
        "        # See the predefined taxonomy of categories:\n",
        "        # https://cloud.google.com/natural-language/docs/categories\n",
        "        print(u\"Category name: {}\".format(category.name))\n",
        "        # Get the confidence. Number representing how certain the classifier\n",
        "        # is that this category represents the provided text.\n",
        "        print(u\"Confidence: {}\".format(category.confidence))"
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}