{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnbltdnw7nw",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper\n",
        "This is a tool to crape Google for the first search result and saving it as a .csv.\n",
        "\n",
        "## Important\n",
        "Please do not modify any code below except for one line that is specified.\n",
        "\n",
        "## Requirements\n",
        "This is Colab, so you don't need anything other than this notebook itself and a Google Drive (or a shared team drive).\n",
        "\n",
        "## Please Make a Copy of this First\n",
        "To ensure everyone can collaborate without stumbling upon each other's progress, I suggest that you first make a copy of this notebook without changing anything. To do so, go to the navigation bar -> File -> Save a Copy in Drive. Then you will be able to find the copy in your Google Drive.\n",
        "\n",
        "## To Run this\n",
        "1. Click the run button on the top left of each code snippet\n",
        "2. For the first snippet to mount Google Drive, click the link after the code runs, copy the authorization code from Google Drive into the box then hit enter.\n",
        "3. After clicking through all of the run buttons before the \"Stop Here\" sign, evaluate what you will be doing and follow the instructions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9CCXhHDI_tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsLhuzDBKJUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bs4 selenium webdriver_manager\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RdiceTSJ0gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "from urllib.parse import quote_plus\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "\n",
        "def read_csv(filename):\n",
        "  file = open(filename, 'r')\n",
        "  reader = csv.reader(file)\n",
        "  # skip the header\n",
        "  next(reader, None)\n",
        "  data = list(reader)\n",
        "  file.close()\n",
        "  return list(data)\n",
        "\n",
        "def fetch_webpage(url, browser):\n",
        "    browser.get(url)\n",
        "    html = browser.page_source\n",
        "    return html\n",
        "\n",
        "def get_google_url(query, num=1, start=0, lang='en'):\n",
        "  query = quote_plus(str(query))\n",
        "  url = 'https://www.google.com/search?q={}&num={}&start={}&nl={}'.format(query, num, start, lang)\n",
        "  return url\n",
        "\n",
        "def save_as_dict(name, link, about):\n",
        "  data_dict = {\n",
        "    \"name\": name,\n",
        "    \"link\": link,\n",
        "    \"about\": about\n",
        "  }\n",
        "  return data_dict\n",
        "\n",
        "def find_link(webpage):\n",
        "  soup = BeautifulSoup(webpage, 'html.parser')\n",
        "  # find a list of all span elements\n",
        "  span = soup.find('span', {'class' : 'ellip'})\n",
        "  if not span:\n",
        "    link = soup.find('cite')\n",
        "    if link:\n",
        "      url = soup.find('cite').get_text()\n",
        "    else:\n",
        "      return '', ''\n",
        "  else:\n",
        "    url = span.get_text()\n",
        "  print(url)\n",
        "\n",
        "  # Find a brief description\n",
        "  data = soup.find('div', {\"class\": 'g'})\n",
        "  if not data:\n",
        "    return '', ''\n",
        "  about = data.find('span',{'class':'st'})\n",
        "  if not about:\n",
        "    return '', ''\n",
        "  about_text = about.text.strip()\n",
        "  print(about_text)\n",
        "  return url, about_text\n",
        "\n",
        "def search_google(data, start, length):\n",
        "  names = data[start: start + length]\n",
        "  # Chrome fails to start on Google Cloud Shell\n",
        "  # browser = webdriver.Chrome(ChromeDriverManager().install())\n",
        "  chrome_options = Options()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  browser = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "  info_dict = []\n",
        "  for name in names:\n",
        "    link = get_google_url(name[0])\n",
        "    print(link)\n",
        "    webpage = fetch_webpage(link, browser)\n",
        "    url, about = find_link(webpage)\n",
        "    info_dict.append(save_as_dict(name[0], url, about))\n",
        "  return info_dict\n",
        "\n",
        "def save_results(results, start, length):\n",
        "  csv_columns = ['name','link','about']\n",
        "  with open('/content/drive/My Drive/Capstone/scraped_data/links_irs990_' + \n",
        "            str(start) + '_' + str(start + length) + '.csv', 'w') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
        "    writer.writeheader()\n",
        "    for data in results:\n",
        "        writer.writerow(data)\n",
        "\n",
        "\n",
        "data = read_csv('/content/drive/My Drive/irs990_names.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5twPDWQ4yojO",
        "colab_type": "text"
      },
      "source": [
        "# Stop Here\n",
        "\n",
        "## Note\n",
        "Colab is able to scrape 50 links in one function without any issues. However, with over 50 queries, the requests sometimes fail. Therefore, the scraping is done 50 entries at a time. Each run saves the result in a csv. You can run multiple scrapes at once (see options below). Ultimately, Google only allows about 1000 queries per day for an IP (but realistically the limit is lower). Therefore, the scraping can be done in batches of 500 entries without failing.\n",
        "\n",
        "## Options\n",
        "1. The first code snippet below runs multiple batches at once. \n",
        "**Instructions:**\n",
        "*  Set the start in increments of 500\n",
        "* If you see the links being printed do not include about/name, then the VM needs to restart\n",
        "* If you see any errors during the run, try restarting the runtime and rerun the same batch. If the problem persists, please describe the error to Tony\n",
        "* After each run is complete, please check on Google Drive that the output is valid, and restart the VM (see instruction below)\n",
        "2. The second code snippet runs a single scrape with only 50 organizations (only used when debuging)\n",
        "\n",
        "## Restart the VM\n",
        "* From the navigate bar, click Runtime -> Factory Reset Runtime\n",
        "* Go back to the top of the notebook and rerun everything before \"Stop Here\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjMhswMCXKGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Click me for larger scrapes\n",
        "# TODO: Set Start in increments of 500\n",
        "start = 10000\n",
        "\n",
        "'''\n",
        "Do not modify below this line\n",
        "-----------------------------------------------\n",
        "'''\n",
        "stop = start + 500\n",
        "step = 50\n",
        "for i in range(start, stop, step):\n",
        "  results = search_google(data, i, step)\n",
        "  save_results(results, i, step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUKscwDAdc3A",
        "colab_type": "text"
      },
      "source": [
        "# Stop and Factory Reset Runtime after Each Iteration of 500 Scrapes\n",
        "Your don't need to run or change anything below this line\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2VKnFLzWzLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For single runs\n",
        "# TODO: set start\n",
        "start = 9500\n",
        "num_entries = 50\n",
        "results = search_google(data, start, num_entries)\n",
        "save_results(results, start, num_entries)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}